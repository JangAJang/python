{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0038eaab",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "- DataSet 관리\n",
    "- DataLoader\n",
    "\n",
    "데이터가 많을 수록 학습할 표본이 많은것이다. \n",
    "하지만, 데이터가 많아질 수록 서버가 느려지며 전체를 load할 수 없을수도 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a902b5",
   "metadata": {},
   "source": [
    "## Minibatch Gradient Descent\n",
    "\n",
    "- 전체 데이터를 균일하게 나눠 학습시키는 방식\n",
    "- 각 Minibatch만큼만 cost를 계산해 경사하강을 이용해 prediction을 계산한다.\n",
    "- 각 업데이트마다 데이터의 수가 적어져 빠르다. \n",
    "- 전체를 쓰지 않아 cost를 그래프화시키면 지진계처럼 흔들려있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9410b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                       [93, 88, 93],\n",
    "                       [89, 91, 90],\n",
    "                       [96, 98, 100],\n",
    "                       [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "dataset = CustomDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7e593",
   "metadata": {},
   "source": [
    "## Pytorch DataLoader\n",
    "\n",
    "- 데이터 로더를 초기화하기 위해서, `Dataset`과 데이터셋을 분할하여 예측 계산하기 위한 `minibatch`의 크기를 입력해야 한다. \n",
    "- 순서대로 학습할 경우, 매번 학습 결과가 같아진다. 이를 위해 shuffle을 True로 가져간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2deb79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b7cc5",
   "metadata": {},
   "source": [
    "- enumerate\n",
    "    - minibatch의 인덱스와 데이터를 받아 순회시킬 때 사용한다. \n",
    "- len(dataloader)\n",
    "    - epoch당 minibatch의 개수를 알려준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7fa923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Hypothesis: tensor([93.1129, 70.0796]), Cost: 6360.955566\n",
      "Epoch    0/20 Hypothesis: tensor([139.4846, 128.3395]), Cost: 3202.196289\n",
      "Epoch    0/20 Hypothesis: 133.30250549316406, Cost: 349.596313\n",
      "Epoch    1/20 Hypothesis: tensor([139.7924, 125.2383]), Cost: 214.989349\n",
      "Epoch    1/20 Hypothesis: tensor([170.0131, 171.2277]), Cost: 144.706314\n",
      "Epoch    1/20 Hypothesis: 191.8470458984375, Cost: 17.247028\n",
      "Epoch    2/20 Hypothesis: tensor([178.0339, 194.2409]), Cost: 3.479938\n",
      "Epoch    2/20 Hypothesis: tensor([136.5596, 152.1379]), Cost: 14.808642\n",
      "Epoch    2/20 Hypothesis: 181.33746337890625, Cost: 13.414174\n",
      "Epoch    3/20 Hypothesis: tensor([138.7309, 198.3184]), Cost: 8.030951\n",
      "Epoch    3/20 Hypothesis: tensor([183.1740, 181.7793]), Cost: 3.250013\n",
      "Epoch    3/20 Hypothesis: 154.5157928466797, Cost: 6.329214\n",
      "Epoch    4/20 Hypothesis: tensor([197.2146, 182.1458]), Cost: 4.810948\n",
      "Epoch    4/20 Hypothesis: tensor([153.9649, 138.2460]), Cost: 8.976678\n",
      "Epoch    4/20 Hypothesis: 181.44903564453125, Cost: 2.099704\n",
      "Epoch    5/20 Hypothesis: tensor([137.9370, 197.1936]), Cost: 8.966442\n",
      "Epoch    5/20 Hypothesis: tensor([182.5842, 154.0020]), Cost: 4.922117\n",
      "Epoch    5/20 Hypothesis: 181.37750244140625, Cost: 1.897513\n",
      "Epoch    6/20 Hypothesis: tensor([180.7079, 182.0934]), Cost: 4.474847\n",
      "Epoch    6/20 Hypothesis: tensor([197.7470, 154.0510]), Cost: 3.629373\n",
      "Epoch    6/20 Hypothesis: 137.65048217773438, Cost: 18.918304\n",
      "Epoch    7/20 Hypothesis: tensor([138.9193, 183.4131]), Cost: 6.004397\n",
      "Epoch    7/20 Hypothesis: tensor([199.6234, 182.9756]), Cost: 10.991416\n",
      "Epoch    7/20 Hypothesis: 154.08242797851562, Cost: 4.336506\n",
      "Epoch    8/20 Hypothesis: tensor([153.3596, 180.4386]), Cost: 1.020460\n",
      "Epoch    8/20 Hypothesis: tensor([137.4118, 196.4409]), Cost: 10.623198\n",
      "Epoch    8/20 Hypothesis: 182.19334411621094, Cost: 7.877317\n",
      "Epoch    9/20 Hypothesis: tensor([154.8306, 182.1830]), Cost: 6.388934\n",
      "Epoch    9/20 Hypothesis: tensor([138.2039, 182.4719]), Cost: 10.400837\n",
      "Epoch    9/20 Hypothesis: 199.0066375732422, Cost: 9.039869\n",
      "Epoch   10/20 Hypothesis: tensor([182.2157, 138.0093]), Cost: 11.839108\n",
      "Epoch   10/20 Hypothesis: tensor([154.8876, 198.8384]), Cost: 8.197412\n",
      "Epoch   10/20 Hypothesis: 180.9102783203125, Cost: 0.828607\n",
      "Epoch   11/20 Hypothesis: tensor([137.7419, 196.8932]), Cost: 9.464651\n",
      "Epoch   11/20 Hypothesis: tensor([181.0322, 182.4374]), Cost: 3.816027\n",
      "Epoch   11/20 Hypothesis: 154.1753387451172, Cost: 4.732099\n",
      "Epoch   12/20 Hypothesis: tensor([181.9195, 153.4203]), Cost: 5.753203\n",
      "Epoch   12/20 Hypothesis: tensor([180.9880, 138.1490]), Cost: 7.903259\n",
      "Epoch   12/20 Hypothesis: 197.98727416992188, Cost: 3.949259\n",
      "Epoch   13/20 Hypothesis: tensor([196.8418, 180.4202]), Cost: 0.442587\n",
      "Epoch   13/20 Hypothesis: tensor([137.4597, 181.4908]), Cost: 16.464485\n",
      "Epoch   13/20 Hypothesis: 154.51016235351562, Cost: 6.300915\n",
      "Epoch   14/20 Hypothesis: tensor([182.1927, 180.7838]), Cost: 4.247603\n",
      "Epoch   14/20 Hypothesis: tensor([138.3853, 154.0620]), Cost: 8.659204\n",
      "Epoch   14/20 Hypothesis: 198.0637969970703, Cost: 4.259258\n",
      "Epoch   15/20 Hypothesis: tensor([137.7402, 196.8742]), Cost: 9.454828\n",
      "Epoch   15/20 Hypothesis: tensor([153.8352, 181.0193]), Cost: 2.203431\n",
      "Epoch   15/20 Hypothesis: 181.80189514160156, Cost: 10.227875\n",
      "Epoch   16/20 Hypothesis: tensor([181.9719, 198.5322]), Cost: 5.150091\n",
      "Epoch   16/20 Hypothesis: tensor([182.2375, 138.0327]), Cost: 11.685122\n",
      "Epoch   16/20 Hypothesis: 154.87144470214844, Cost: 8.245194\n",
      "Epoch   17/20 Hypothesis: tensor([197.5513, 182.4926]), Cost: 4.346786\n",
      "Epoch   17/20 Hypothesis: tensor([154.0498, 138.3899]), Cost: 8.617193\n",
      "Epoch   17/20 Hypothesis: 181.5365447998047, Cost: 2.360970\n",
      "Epoch   18/20 Hypothesis: tensor([153.6357, 180.7897]), Cost: 1.649617\n",
      "Epoch   18/20 Hypothesis: tensor([181.6746, 196.6709]), Cost: 5.754153\n",
      "Epoch   18/20 Hypothesis: 138.10362243652344, Cost: 15.181758\n",
      "Epoch   19/20 Hypothesis: tensor([198.9662, 183.8163]), Cost: 5.099786\n",
      "Epoch   19/20 Hypothesis: tensor([154.5509, 138.8591]), Cost: 8.186392\n",
      "Epoch   19/20 Hypothesis: 181.943603515625, Cost: 3.777595\n",
      "Epoch   20/20 Hypothesis: tensor([182.4272, 138.1817]), Cost: 10.599428\n",
      "Epoch   20/20 Hypothesis: tensor([182.3512, 198.9463]), Cost: 7.104533\n",
      "Epoch   20/20 Hypothesis: 153.8081512451172, Cost: 3.269411\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultivariableLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = MultivariableLinearRegression()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    for batch_idx, sample in enumerate(dataloader):\n",
    "        x_train, y_train = sample\n",
    "        prediction = model(x_train)\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Hypothesis: {}, Cost: {:.6f}'.format(epoch,nb_epochs,prediction.squeeze().detach(),cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432293d9",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "- 숫자 하나의 답이 아닌, 분류를 위한 모델을 학습한다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
