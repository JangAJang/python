{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5891359",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "- Gradient Vanishing / Exploding\n",
    "- 그 원인 : Internal Covariate Shift\n",
    "- 해결책 : Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d173bcb",
   "metadata": {},
   "source": [
    "## Gradient Vanishing / Exploding\n",
    "\n",
    "- Vanishing은 ReLU공부할 때 한 번 나왔었다. \n",
    "- 기울기가 너무 작아져 사라지는 과정\n",
    "- Exploding : gradient가 너무 커지는 상황"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5cf2f",
   "metadata": {},
   "source": [
    "## Gradient Vanishing문제를 해결하기 위한 방법들\n",
    "\n",
    "- Change activation function : ReLU와 같은 다른 활성함수 사용\n",
    "- Careful initialization : `weight_initialization`에서 배운 것 과 같이 처음에 잘 초기화시키자. \n",
    "- Small learning rate : lr을 낮추어 완화시킬 수 있다. \n",
    "\n",
    "다만, 이 방식들은 간접적인 Gradient vanishing을 해결하는 것이며, `Batch normalization`을 이용하는 직접적인 해결책이 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9851563",
   "metadata": {},
   "source": [
    "## Internal Covariate Shift \n",
    "\n",
    "![internal covariate shift.png](/python/pytorch_study/images/internal%20covariate%20shift.png)\n",
    "\n",
    "- Covariate shift : neural network에서 train set, test set 간의 분포 차이가 있는 것\n",
    "- neural network에서 초기의 train set, test set간의 분포 차이가 없더라도 레이어가 쌓여갈 수록 input, output간의 분포 차이가 커진다. \n",
    "- 결국 레이어 간의 Covariate shift가 발생한다. 그리고 레이어가 많을수록 더 커진다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
