{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Array with Numpy\n",
    "- 1차원 배열을 numpy를 통해 초기화한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6.]\n",
      "Rank of t :  1\n",
      "Shape of t :  (7,)\n"
     ]
    }
   ],
   "source": [
    "t = np.array([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)\n",
    "print('Rank of t : ', t.ndim)\n",
    "print('Shape of t : ', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Array with Numpy\n",
    "- 2차원 배열(행렬)을 numpy를 통해 초기화한다.\n",
    "- Rank는 차원, Shape은 x, y축의 크기를 알려준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]]\n",
      "Rank of t :  2\n",
      "Shape of t :  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
    "print(t)\n",
    "print('Rank of t : ', t.ndim)\n",
    "print('Shape of t : ', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Array with PyTorch\n",
    "- dim : 차원\n",
    "- shape : 차원당 원소 개수\n",
    "- size : 차원당 원소 개수(=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])\n",
    "print(t)\n",
    "\n",
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())\n",
    "print(t[0], t[1], t[-1])\n",
    "print(t[2:5], t[4:-1])\n",
    "print(t[:2], t[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Array with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "2\n",
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\n",
    "\n",
    "print(t)\n",
    "\n",
    "print(t.dim())\n",
    "print(t.shape)\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BroadCasting\n",
    "- 행렬 의 덧셈, 뺄셈에선 두 Tensor간의 크기가 같아야 한다. \n",
    "- 행렬곱을 수행할 때, 마지막 차원과 첫 차원이 일치해야 한다. \n",
    "- 하지만 불가피하게 다른 크기 행렬간의 사칙연산이 필요해진다. \n",
    "- PyTorch의 BroadCasting은 자동적으로 차원을 맞춰준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5.]])\n",
      "tensor([[4., 5.]])\n",
      "tensor([[4., 5.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.FloatTensor([[3, 3]])\n",
    "a2 = torch.FloatTensor([[2, 2]])\n",
    "\n",
    "print(a1 + a2)\n",
    "\n",
    "b1 = torch.FloatTensor([[1, 2]])\n",
    "b2 = torch.FloatTensor([3])\n",
    "\n",
    "print(b1 + b2)\n",
    "\n",
    "c1 = torch.FloatTensor([[1, 2]])\n",
    "c2 = torch.FloatTensor([[3], [4]])\n",
    "\n",
    "print(c1 + c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplication vs Matrix Multiplication\n",
    "- 자동적인 동작이 되기 때문에 휴먼 에러를 조심해야 한다. \n",
    "- 에러 없이 BroadCasting이 되기 때문에 디버깅이 어렵다. \n",
    "- 사칙연산의 `*`를 사용하면 행렬곱이 아닌, 요소 곱으로 나온다. \n",
    "- 행렬곱을 사용할 때 `@`를 사용하면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Mul vs Matmul\n",
      "----------\n",
      "shape of m1 :  torch.Size([2, 2])\n",
      "shape of m2 :  torch.Size([2, 1])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n",
      "tensor([[ 5.],\n",
      "        [11.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n",
      "tensor([[1., 2.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print('----------\\nMul vs Matmul\\n----------')\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print('shape of m1 : ', m1.shape)\n",
    "print('shape of m2 : ', m2.shape)\n",
    "print(m1 @ m2)\n",
    "print(m1.matmul(m2))\n",
    "m1 = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "m2 = torch.FloatTensor([[1], [2]])\n",
    "print(m1 * m2)\n",
    "print(m1.mul(m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean(평균)\n",
    "- Long타입의 경우 평균을 구하지 못하는 경우가 존재한다. \n",
    "- `dim`파라미터를 통해 특정 차원에 대한 평균값을 구할 수 있다. \n",
    "- `Sum`에 대해서도 같은 방식으로 동작하게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long\n",
      "tensor(2.5000)\n",
      "tensor([2., 3.])\n",
      "tensor([1.5000, 3.5000])\n",
      "tensor([1.5000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    t = torch.LongTensor([1, 2])\n",
    "    print(t.mean())\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "\n",
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t.mean())\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=1))\n",
    "print(t.mean(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max and ArgMax\n",
    "- max는 행렬에서 최대값을 찾아준다. \n",
    "- 뒤에 dim을 붙일 수 있다. \n",
    "- dim=0일 때 [2, 1]크기로 만들어준다.(세로로 최대값 비교를 해 가장 큰값을 모은 행렬로 만들어준다. )\n",
    "- dim=1일 때 [1, 2]크기로 만들어준다. \n",
    "- 이 때 max에 인덱스를 붙여 확인이 가능하다. dim에 대해 max를 구하면, 0인덱스는 최대값행렬을, 1인덱스는 최대값행렬에 들어간 원소의 각 행/렬 상의 인덱스를 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor(4.)\n",
      "tensor([3., 4.])\n",
      "tensor([1, 1])\n",
      "tensor([2., 4.])\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "t = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "print(t)\n",
    "print(t.max())\n",
    "print(t.max(dim=0)[0])\n",
    "print(t.max(dim=0)[1])\n",
    "print(t.max(dim=1)[0])\n",
    "print(t.max(dim=1)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View\n",
    "- `view`는 `reshape`와 동일한 동작을 수행한다. \n",
    "- 총 원소 개수는 유지하면서 파라미터로 지정한 형태에 따라 재구성한다. \n",
    "- `view`의 파라미터로 배열이 들어가며, 입력된 값의 크기를 고정값으로 행렬을 재구성한다. \n",
    "- 이 때 `-1`이 입력되면 해당 값은 다른 크기에 따라 유동적으로 크기를 설정한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.]])\n",
      "torch.Size([4, 3])\n",
      "tensor([[[ 0.,  1.,  2.]],\n",
      "\n",
      "        [[ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.]]])\n",
      "torch.Size([4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]])\n",
    "ft = torch.FloatTensor(t)\n",
    "\n",
    "print(ft.shape)\n",
    "\n",
    "print(ft.view([-1, 3]))\n",
    "print(ft.view([-1, 3]).shape)\n",
    "\n",
    "print(ft.view([-1, 1, 3]))\n",
    "print(ft.view([-1, 1, 3]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze\n",
    "- 크기가 1인 차원을 모두 제거한다. \n",
    "- 구조를 단순화시킬 때 이용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([0., 1., 2.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([[0], [1], [2]])\n",
    "print(ft)\n",
    "print(ft.shape)\n",
    "\n",
    "print(ft.squeeze())\n",
    "print(ft.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsqueeze\n",
    "\n",
    "- 차원 확장에 이용한다.\n",
    "- 특정 차원에 크기가 1인 차원을 추가해준다. \n",
    "- 그러면 unsqueeze하고 나서 squeeze하면 다시 원복될까?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([[0., 1., 2.]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([1, 2, 3])\n",
      "torch.Size([2, 1, 3])\n",
      "torch.Size([2, 3, 1])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "ft = torch.FloatTensor([0, 1, 2])\n",
    "print(ft.shape)\n",
    "\n",
    "print(ft.unsqueeze(0))\n",
    "print(ft.unsqueeze(0).shape)\n",
    "print(ft.unsqueeze(1).shape)\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # shape: (2,3)\n",
    "print(x.shape)\n",
    "\n",
    "x_unsqueezed = torch.unsqueeze(x, 0)  # 첫 번째 차원에 추가\n",
    "print(x_unsqueezed.shape)  # torch.Size([1, 2, 3])\n",
    "\n",
    "x_unsqueezed = torch.unsqueeze(x, 1)  # 두 번째 차원에 추가\n",
    "print(x_unsqueezed.shape)  # torch.Size([2, 1, 3])\n",
    "\n",
    "x_unsqueezed = torch.unsqueeze(x, 2)  # 세 번째 차원에 추가\n",
    "print(x_unsqueezed.shape)  # torch.Size([2, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Casting\n",
    "- 타입별 함수를 통해 텐서의 타입을 변환해줄 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "tensor([1, 0, 0, 1])\n",
      "tensor([1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "lt = torch.LongTensor([1, 2, 3, 4])\n",
    "\n",
    "print(lt.float())\n",
    "bt = torch.ByteTensor([True, False, False, True])\n",
    "print(bt.long())\n",
    "print(bt.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate\n",
    "- 텐서를 연결할 수 있다. \n",
    "- dim을 파라미터로 받아 확장시킨다 ex. (2, 2), (2, 2)인 텐서를 cat할 때 dim = 0이라면 (4, 2)가. dim=1이라면 (2,4)가 된다. \n",
    "- 텐서에 존재하지 않는 차원을 파라미터로 주면 에러\n",
    "- dim으로 연결되는 행렬 값의 수가 같아야 한다\n",
    "    - (2, 2), (3, 2)의 벡터를 dim=0에 대해 concatenate한다면 (5, 2)가 되지만 dim=1에 대해 concatinate할 수 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "tensor([[ 1.,  2.],\n",
      "        [ 3.,  4.],\n",
      "        [ 5.,  6.],\n",
      "        [ 7.,  8.],\n",
      "        [ 9., 10.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 2 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcat([x, y], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 2 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2], [3, 4]])\n",
    "y = torch.FloatTensor([[5, 6], [7, 8]])\n",
    "\n",
    "print(torch.cat([x, y], dim=0))\n",
    "print(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "- 여러 텐서를 새로운 차원에 쌓는다. \n",
    "- cat은 행렬값을 합친 결과로 같은 차원의 크기가 다른 벡터를 반환하지만, stack은 차원이 변경된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([1, 4])\n",
    "y = torch.FloatTensor([2, 5])\n",
    "z = torch.FloatTensor([3, 6])\n",
    "\n",
    "print(torch.stack([x, y, z]))\n",
    "print(torch.stack([x, y, z]).shape)\n",
    "print(torch.stack([x, y, z], dim=1))\n",
    "print(torch.stack([x, y, z], dim=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ones and Zeros\n",
    "- 기존 텐서와 같은 크기와 데이터타입을 가지며 모든 값이 0(zeros_like), 1(ones_like)인 텐서를 반환한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\n",
    "\n",
    "print(torch.ones_like(x))\n",
    "print(torch.zeros_like(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place Operation\n",
    "- 텐서에 대한 원소곱을 수행한다. \n",
    "- `mul`로 수행할 경우 원소곱을 반환하지만 실제 텐서에는 영향을 주지 않는다. \n",
    "- `mul_`로 수행할 경우 실제 텐서에 원소곱을 해준 후 해당 텐서를 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 4.],\n",
      "         [6., 8.]]])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]]])\n",
      "tensor([[[2., 4.],\n",
      "         [6., 8.]]])\n",
      "tensor([[[2., 4.],\n",
      "         [6., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[[1, 2], [3, 4]]])\n",
    "\n",
    "print(x.mul(2.0))\n",
    "print(x)\n",
    "print(x.mul_(2.0))\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
