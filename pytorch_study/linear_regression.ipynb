{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Definition\n",
    "\n",
    "## 학습 예제\n",
    "- 선형회귀를 이용한 공부 시간과 점수의 상관관계 예측\n",
    "- Train Set, Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "```\n",
    "y = Wx + b\n",
    "```\n",
    "W : 가중치, b = bias\n",
    "\n",
    "- 초기 W, b를 0으로 초기화한다. \n",
    "    - 항상 출력을 0인 상태로 예측한다. \n",
    "- requires_grad=True\n",
    "    - 학습용 데이터임을 명시한다. \n",
    "\n",
    "> TensorFlow에 비해서 구현 방식이 더 직관적인 느낌이 든다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute loss\n",
    "\n",
    "- 선형회귀에선 `MSE`로 loss를 계산한다. \n",
    "- MSE = Mean Squared Error : 오차제곱평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean(hypothesis - y_train ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.optim 라이브러리를 이용해 최적화 알고리즘 라이브러리 호출\n",
    "- SGD : 확률적 경사하강법 알고리즘을 이용해 가중치를 계산한다. \n",
    "- lr : learning rate(학습률), 한 번의 학습으로 가중치를 변경할 크기를 결정한다. \n",
    "- zero_grad : 이전 단계의 gradient를 0으로 초기화시킨다. \n",
    "- backward : 역전파를 이용해 gradient를 계산한다. \n",
    "- step : 계산된 gradient를 이용해 W, b를 업데이트 해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# cost.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 학습시에는 epoch을 이용해 학습을 반복시킨다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(0, nb_epochs):\n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "            \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
