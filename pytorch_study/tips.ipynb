{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957882fd",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "- 관측 데이터가 가장 그럴듯하게 나올 수 있는 모델 파라미터를 찾는 방법\n",
    "- 특정 데이터 x에 대해 파라미터 θ 하에서 데이터가 나올 확률이 나올 때, 이를 최대화시키는 θ를 찾는 것\n",
    "\n",
    "θ_MLE = argmax L(θ | x)\n",
    "- L(θ | x) : Likelihood Function으로, 파라미터 θ 하에서 x가 나올 확률\n",
    "- θ_MLE : MLE로 추정한 최적 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fd483",
   "metadata": {},
   "source": [
    "베르누이 분포를 이용해 예시를 들어보자. \n",
    "\n",
    "압정을 100번 던져서 머리와 다리 중, 머리로 떨어진 횟수가 27번이다. \n",
    "이 때 L(θ | x) = L(p) = p ^ 27 * (1 - p) ^ (100 - 27) 으로 나온다. \n",
    "\n",
    "p에 따른 L(p)를 최대화 하기 위한 θ는 어떻게 구할까? \n",
    "\n",
    "1. 로그를 씌운다. \n",
    "    logL(p) = 27logp + 73log(1-p)\n",
    "\n",
    "2. 미분을 하고 그 값을 0으로 놓는다(미분시 0은 최대, 최소값을 의미함)\n",
    "    27 / p - 73 / (1-p) = 0, 27/p = 73/(1-p)\n",
    "\n",
    "3. p를 구한다. \n",
    "    73p = 27 - 27p, p = 0.27\n",
    "\n",
    "이번 경우에서 최적의 파라미터는 0.27이 나온다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a83948",
   "metadata": {},
   "source": [
    "MLE는 확률 모델을 정하고, L(p)를 정의한 후 Likelihood를 최대화하는 파라미터를 찾는 방법이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5b6d0",
   "metadata": {},
   "source": [
    "# Overfitting(과적합)\n",
    "\n",
    "과적합시, 테스트 데이터에 대해서는 원하는 결과를 얻을 수 있지만, 다른 데이터에서는 정확한 답이 나오지 않을 수 있다. \n",
    "\n",
    "그렇다면 어떻게 과적합을 예방할 수 있을까? \n",
    "- 데이터 크기 증가\n",
    "- feature를 줄이기\n",
    "- Regularization를 이용해 과적합 방지\n",
    "    - Early Stopping : 중간에 validation loss가 낮아지지 않을 때 멈추는 것\n",
    "    - Reducing Network Size : 뉴럴 네트워크의 사이즈를 줄이는 것\n",
    "    - Weight Decay : W 파라미터의 크기 제한\n",
    "    - Droptout : 딥러닝시 뉴런을 일부 랜덤으로 꺼버리는 방식(뉴런 의존성을 줄일 수 있다)\n",
    "    - Batch Normalization : 각 레이어의 출력을 평균 0, 분산을 1로 맞추는 방식\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a48bad",
   "metadata": {},
   "source": [
    "# Basic Approach to train DNN\n",
    "\n",
    "1. 뉴럴 네트워크를 설계한다. ex) 입력 데이터가 vector이며 10개의 feature가 있다면, 10차원의 입력을 받아 이를 토대로 5가지의 클래스로 만들어야 한다면 softmax 10 -> 5가 나온다. \n",
    "2. 훈련 및 과적합 체크\n",
    "    - 만약 과적합이 아니라면 모델 사이즈를 증가시킨다. \n",
    "    - 과적합이 발생한다면 Regularization을 추가한다. \n",
    "3. 2번을 반복한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f41f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x146fe6469d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e772c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1, 2, 1], \n",
    "                             [1, 3, 2],\n",
    "                             [1, 3, 4],\n",
    "                             [1, 5, 5],\n",
    "                             [1, 7, 5],\n",
    "                             [1, 2, 2],\n",
    "                             [1, 6, 6],\n",
    "                             [1, 7, 7]])\n",
    "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f8518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29798d96",
   "metadata": {},
   "source": [
    "shape을 통해서, 우리가 해야하는 것이 classification이라는 것을 알 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e076598",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "y_test = torch.LongTensor([2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc18d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb2d5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e3b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24de2f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    nb_epochs = 20\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        prediction = model(x_train)\n",
    "        cost = F.cross_entropy(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d} / {} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae85f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test (validation)\n",
    "\n",
    "def test (model, optimizer, x_test, y_test):\n",
    "    prediction = model(x_test)\n",
    "    predicted_classes = prediction.max(1)[1]\n",
    "    correct_count = (predicted_classes == y_test).sum().item()\n",
    "    cost = F.cross_entropy(prediction, y_test)\n",
    "    \n",
    "    print('Accuracy: {}% Cost: {:.6f}'.format(correct_count / len(y_test) * 100, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb0a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 / 20 Cost: 2.272475\n",
      "Epoch    1 / 20 Cost: 1.250806\n",
      "Epoch    2 / 20 Cost: 1.199265\n",
      "Epoch    3 / 20 Cost: 1.174748\n",
      "Epoch    4 / 20 Cost: 1.158751\n",
      "Epoch    5 / 20 Cost: 1.146726\n",
      "Epoch    6 / 20 Cost: 1.136171\n",
      "Epoch    7 / 20 Cost: 1.126433\n",
      "Epoch    8 / 20 Cost: 1.117174\n",
      "Epoch    9 / 20 Cost: 1.108253\n",
      "Epoch   10 / 20 Cost: 1.099598\n",
      "Epoch   11 / 20 Cost: 1.091173\n",
      "Epoch   12 / 20 Cost: 1.082957\n",
      "Epoch   13 / 20 Cost: 1.074937\n",
      "Epoch   14 / 20 Cost: 1.067105\n",
      "Epoch   15 / 20 Cost: 1.059454\n",
      "Epoch   16 / 20 Cost: 1.051978\n",
      "Epoch   17 / 20 Cost: 1.044672\n",
      "Epoch   18 / 20 Cost: 1.037530\n",
      "Epoch   19 / 20 Cost: 1.030549\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775cabe",
   "metadata": {},
   "source": [
    "파라미터가 optimizer를 통해 MLE를 계산하며 최적값을 구해지고 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f456bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0% Cost: 1.518564\n"
     ]
    }
   ],
   "source": [
    "test(model, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648cf22a",
   "metadata": {},
   "source": [
    "LOSS값이 늘어난 상태이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0c293",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "lr이 너무 크면 diverge하면서 cost가 점점 증가한다. \n",
    "lr이 너무 작으면 cost가 거의 줄어들지 않는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd2bd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 / 20 Cost: 1.312794\n",
      "Epoch    1 / 20 Cost: 1.011654\n",
      "Epoch    2 / 20 Cost: 1.005033\n",
      "Epoch    3 / 20 Cost: 0.998682\n",
      "Epoch    4 / 20 Cost: 0.992507\n",
      "Epoch    5 / 20 Cost: 0.986491\n",
      "Epoch    6 / 20 Cost: 0.980623\n",
      "Epoch    7 / 20 Cost: 0.974895\n",
      "Epoch    8 / 20 Cost: 0.969301\n",
      "Epoch    9 / 20 Cost: 0.963836\n",
      "Epoch   10 / 20 Cost: 0.958496\n",
      "Epoch   11 / 20 Cost: 0.953276\n",
      "Epoch   12 / 20 Cost: 0.948173\n",
      "Epoch   13 / 20 Cost: 0.943184\n",
      "Epoch   14 / 20 Cost: 0.938304\n",
      "Epoch   15 / 20 Cost: 0.933531\n",
      "Epoch   16 / 20 Cost: 0.928862\n",
      "Epoch   17 / 20 Cost: 0.924293\n",
      "Epoch   18 / 20 Cost: 0.919822\n",
      "Epoch   19 / 20 Cost: 0.915446\n"
     ]
    }
   ],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "train(model, optimizer, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "603a6c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0% Cost: 0.428293\n"
     ]
    }
   ],
   "source": [
    "test(model, optimizer, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0172664",
   "metadata": {},
   "source": [
    "# Data Preprocessing(데이터 전처리)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f905a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fad1c9",
   "metadata": {},
   "source": [
    "값이 특정 classes로 만들어지는 것은 아니다. \n",
    "regression을 시도해볼 수 있다. \n",
    "그렇다면 mean-square-error를 이용할 수 있다. \n",
    "\n",
    "이 때 데이터 전처리를 하면 계산이 수월해진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3306840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = x_train.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cf3fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84.8000, 84.6000, 85.6000])\n",
      "tensor([11.0544, 12.2393, 12.6214])\n"
     ]
    }
   ],
   "source": [
    "sigma = x_train.std(dim=0)\n",
    "print(mu)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bec185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0674, -0.3758, -0.8398],\n",
      "        [ 0.7418,  0.2778,  0.5863],\n",
      "        [ 0.3799,  0.5229,  0.3486],\n",
      "        [ 1.0132,  1.0948,  1.1409],\n",
      "        [-1.0674, -1.5197, -1.2360]])\n"
     ]
    }
   ],
   "source": [
    "norm_x_train = (x_train - mu) / sigma\n",
    "print(norm_x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d45cd0",
   "metadata": {},
   "source": [
    "Standardazation(정규화)를 이용해 데이터를 전처리해줄 수 있다. \n",
    "위의 데이터가 정규분포를 따른다고 가정하고 mu, sigma를 이용해 전처리를 진행해준다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
