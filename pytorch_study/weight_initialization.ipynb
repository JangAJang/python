{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28173791",
   "metadata": {},
   "source": [
    "# Weight Initialization \n",
    "\n",
    "- 왜 가중치 초기화가 중요한가?\n",
    "- RBM을 이용해 가중치를 초기화하기 -> DBN\n",
    "- Xavier / He Initialization : 가중치 초기화법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cfaa4",
   "metadata": {},
   "source": [
    "## Geoffrey Hinton's summary of findings up to today\n",
    "\n",
    "- 가중치를 잘 초기화해야하는데, 이에 대해 무지했다\n",
    "- 가중치가 기댓값에서 동떨어진 상태에서 학습을 시작하면 학습에 소요되는 시간이 늘어나며, 실제 성능에도 영향을 미친다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e60292",
   "metadata": {},
   "source": [
    "## Need to set the initial weight values wisely\n",
    "\n",
    "- 처음부터 가중치를 전부 0으로 하면, neural network에서 가중치 업데이트를 위해 back-propagation시 모든 gradient가 0이 된다. \n",
    "- 어떻게 가중치를 초기에 0이 아니게 해서 바로 학습이 제대로 되게 할까?\n",
    "- 이를 위해 `Restricted Boltzmann Machine`이라는 것을 만들어냈다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bcc1a",
   "metadata": {},
   "source": [
    "## RBM\n",
    "\n",
    "- 레이어 안의 노드들 간의 연결은 존재하지 않다. \n",
    "- 다른 레이어에 존재하는 노드 간에는 fully connected상태이다. (a레이어의 모든 노드는 각자 b레이어의 모든 노드와 연결되어있어야 한다) \n",
    "\n",
    "\n",
    "### How can we use RBM to initialize weights?\n",
    "\n",
    "- Pre-training\n",
    "    - 처음엔 두 개의 레이어를 RBM으로 학습시킨다. (input x가 들어가 output y가 나올 때, y의 backward로 x'으로 x를 복원할 수 있게 한다)\n",
    "    - 이후 하나의 레이어를 더 쌓아 RBM학습을 시킨다. \n",
    "        - 이 때 기존 레이어와의 파라미터와 가중치는 fix시킨다. \n",
    "    - 쌓고 RBM으로 학습시킨 후 파라미터 고정시키는 것을 마지막 레이어까지 반복한다. \n",
    "- Fine-tuning\n",
    "    - RBM으로 학습시킨 여러 레이어가 존재한다. \n",
    "    - nueral-network의 일반적인 학습 방식으로 파라미터를 초기화시키는 fine-tuning을 진행한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed37da",
   "metadata": {},
   "source": [
    "## Xavier / He Initialization\n",
    "\n",
    "### Xavier initialization\n",
    "\n",
    "Normal distribution으로 intialization(정규 분포를 이용한 초기화)\n",
    "- W∼N(0,σ^2)\n",
    "- `initializer = tf.random.normal(shape=[in_dim, out_dim], mean=0.0, stddev=0.05)`\n",
    "- `torch.nn.init.normal_(tensor, mean=0.0, std=0.05)`\n",
    "\n",
    "Uniform distribution으로 initialization(균등 분포를 이용한 초기화)\n",
    "- W∼U(a,b)\n",
    "- `initializer = tf.random.uniform(shape=[in_dim, out_dim], minval=-0.05, maxval=0.05)`\n",
    "- `torch.nn.init.uniform_(tensor, a=-0.05, b=0.05)`\n",
    "\n",
    "### He initialization(ReLU에 적합한 정규분포 기반 초기화)\n",
    "\n",
    "He Normal Initialization\n",
    "- initializer = tf.keras.initializers.HeNormal()\n",
    "- torch.nn.init.kaiming_normal_(tensor, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "He Uniform Initialization\n",
    "- initializer = tf.keras.initializers.HeUniform()\n",
    "- torch.nn.init.kaiming_uniform_(tensor, mode='fan_in', nonlinearity='relu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
