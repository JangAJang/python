{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14f808b",
   "metadata": {},
   "source": [
    "# ReLU && Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605d600",
   "metadata": {},
   "source": [
    "## sigmoid의 문제점\n",
    "\n",
    "#### sigmoid를 activation function으로 사용한다면 어떻게 될까?\n",
    "\n",
    "- weight, input를 이용한 sigmoid함수로 output을 구한다. \n",
    "- 실제 정답과의 차이를 이용해 Loss를 구한다. \n",
    "- Loss를 미분하여 gradient를 구한다. \n",
    "- back propagation을 이용해 weight를 업데이트 한다. \n",
    "\n",
    "#### 무엇이 문제일까? \n",
    "\n",
    "- gradient를 구할 때 문제가 된다. \n",
    "- sigmoid함수 그래프를 보면 이해가 쉬워진다. \n",
    "\n",
    "![sigmoid](../statics/image/sigmoid.jpg)\n",
    "\n",
    "- x가 0에 가까울수록 gradient가 확연하게 바뀐다. \n",
    "- 양 끝의 경우, 소수점 뒤의 수만 작게 변하게 된다. \n",
    "- 문제는, back propagation을 이용해 gradient를 전파하고 activation function에서 전파받은 gradient가 무의미해진다(vanishing gradient)\n",
    "- sigmoid함수를 이용하는 레이어가 중첩될수록 더 큰 영향을 끼치게 된다. \n",
    "\n",
    "#### ReLU는 무엇이 다르길래 괜찮은걸까?\n",
    "\n",
    "`f(x) = max(0, x)`\n",
    "\n",
    "- ReLU는 0보다 작으면 무조건 0을, 크면 x를 그대로 반환한다. \n",
    "- x > 0 일 경우, gradient는 항상 1이다. \n",
    "- 음수의 영역에선 gradient가 항상 0이기 떄문에, 음수의 x에 대해선 vanishing gradient가 여전히 발생한다. \n",
    "- pytorch에서는 `torch.nn.relu(x)`를 이용한다. \n",
    "\n",
    "#### 다양한 활성화 함수\n",
    "\n",
    "- sigmoid\n",
    "    - 우리가 아는 그것. 1 / (1 + e^(-x))\n",
    "- tanh\n",
    "    - (e^x - e(-x)) / (e^x + e^(-x))\n",
    "    - sigmoid보다 출력 중심이 0이라 성능을 개선할 수 있다. \n",
    "    - vanishing gradient가 여전히 존재한다. \n",
    "- relu\n",
    "    - 현재 가장 상용화되어있으며, 양수인 x에 항상 사용 가능하다. \n",
    "    - 입력이 0보다 작으면 dead neuron이 발생할 수 있다. \n",
    "- leaky_relu\n",
    "    - f(x) = max(ax, x) a에 따라 음수의 영역에서도 아주 작게나마 gradient를 가진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc17db",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "- 손실함수의 값을 줄이기 위해 가중치를 어떻게 업데이트할까?를 결정하는 역할\n",
    "- torch.optim에 optimizer가 다양하게 존재한다. \n",
    "\n",
    "#### 최적화 알고리즘의 종류\n",
    "- torch.optim.SGD(Stochastic Gradient Descent)\n",
    "    - 한 번에 하나의 샘플 데이터를 이용한다. \n",
    "    - 간단하다\n",
    "    - 학습 튜닝이 필요하며, 수렴까지 오래걸릴 수 있다. \n",
    "- torch.optim.Adagrad\n",
    "    - 과거 기울기를 누적해 learning rate를 자동 조정한다.  \n",
    "    - 학습 데이터가 적을 때 효과적이다. \n",
    "    - lr이 점점 0에 수렴하여 학습을 멈출 수 있다. \n",
    "- torch.optim.Adadelta\n",
    "    - Adadelta를 개선하여, learning rate를 0이 되지 않도록 RMS기반 조정을 수행한다.( Root Mean Square : 기울기 제곱의 평균값의 루트)\n",
    "    - learing rate를 튜닝할 필요가 없다. \n",
    "    - 구조가 복잡하다.\n",
    "- torch.optim.RMSprop\n",
    "    - Adadelta와 유사하다. 최근 기울기에 더 큰 가중치를 둬 학습률을 조정한다. \n",
    "    - RNN에 강하다. (RNN이 뭘까?)\n",
    "    - 모멘텀 조합이 필요할 수 있다. (모멘텀이 뭘까?) \n",
    "- torch.optim.Adam\n",
    "    - 모멘텀 + RMSprop조합으로, 가장 널리 사용된다.\n",
    "    - 수렴이 빠르고 튜닝이 거의 필요 없다. \n",
    "    - 간혹 촤솟값 근처에서 진동형 변동이 일어난다. (와리가리)\n",
    "- torch.optim.Adamax\n",
    "    - Adam 변형으로, 무한노름기반 안정화된 버전이다. (무한노름이 뭘까?)  \n",
    "    - 고차원 공간에 최적화되어있다. \n",
    "    - 일반 Adam보다 사용 빈도가 낮다. \n",
    "- torch.optim.SparseAdam\n",
    "    - sparse tensor용 Adam이다. (sparse tensor가 뭘까?)\n",
    "    - 회소 벡터 최적화에 강하다. \n",
    "    - Dense에는 부적합하다. (Dense가 뭘까?)\n",
    "- torch.optim.ASGD\n",
    "    - 평균 파라미터를 이용한 SGD의 변형버전\n",
    "    - 수렴 안정성이 증가한다. \n",
    "    - 잘 안쓴다. \n",
    "- torch.optim.LBFGS\n",
    "    - 2차 최적화(준 뉴턴 방식), 배치 전체 기반 (준 뉴턴방식이 뭘까?)\n",
    "    - 작은 모델에선 빠른 수렴을 할 수 있다. \n",
    "    - 메모리 부하가 크고, 미니배치에 부적합하다\n",
    "- torch.optim.Rprop\n",
    "    - 기울기 크기 무시하고 방향만을 이용한다(단위 벡터 쓰는건가?)\n",
    "    - 고정된 step size 기반이다. \n",
    "    - SGD보다 느리고, 잘 쓰지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9666d",
   "metadata": {},
   "source": [
    "공부하다보니, 또 다른 모르는 단어들이 나왔다. 한번 찾아봐야겠다. \n",
    "\n",
    "- RNN\n",
    "    - 순환 신경망\n",
    "    - 추후 강의로 존재하니 그때 공부해도 괜찮을 듯 하다\n",
    "- 모멘텀\n",
    "    - 관성의 개념을 도입한 최적화 알고리즘\n",
    "    - SGD에서 다음 스텝의 최적화 알고리즘을 계산할 때 마다 이전 스텝의 이동방향을 일정 비율로 이어가며 관성을 적용\n",
    "    - 일련된 방향으로는 빠르게 이동하며 진동형으로 발생하는 그래프를 억제할 수 있다. \n",
    "    - RMSprop에서 각 파라미터의 learing rate를 적절히 조절하지만 기울기의 방향성에 대한 누적 정보가 존재하지 않는다. \n",
    "    - 이로 인해 진동형 그래프가 발생할 수 있고, 이를 방지하기 위해 모멘텀을 조합하여 사용한다. \n",
    "- 무한노름\n",
    "    - 벡터의 각 원소의 절대값중 가장 큰 값\n",
    "    - 이를 통해 상한을 억제할 수 있다. \n",
    "    - 위에서 `무한 노름 기반 안정화 기법`이라는 건 최대값을 제한해 전체 시스템의 불안정성을 줄이는 기법\n",
    "- sparse tensor\n",
    "    - 희소 텐서\n",
    "    - 대부분이 원소가 0으로 되어있을 경우, 해당 0의 값이 어디있는지를 따로 정리하여 저장한다. \n",
    "    - sparse tensor의 경우 행렬의 크기, 0이 아닌 원소의 인덱스와 값을 저장한다. \n",
    "- Dense(위에 있음)\n",
    "- 준 뉴턴 방식\n",
    "    - 비선형 모델에 대한 최적화 알고리즘중 하나로 뉴턴방법을 일부 활용한다. \n",
    "    - 뉴턴 방식\n",
    "        - x_(k+1) = x_k - H^(-1) d( f(x_k) ) / dx\n",
    "        - 현재 위치 x_k의 기울기와 이차미분 행렬의 역행렬을 곱해 현재 위치 x_k에서 빼는 식으로 가중치 최적화값을 계산한다. \n",
    "        - 헤시안 계산이 비용이 크거나 불가능할 수 있으며, 차원이 커질수록 계산량이 늘어난다.\n",
    "    - 헤시안 계산\n",
    "        - 헤시안 행렬이란 함수를 두 번 편미분한 값들의 행렬이다. \n",
    "        - 즉, 차원이 커질수록(변수의 종류가 많아지겠죠?) 편미분할 거리들이 많아지며 연산량이 증가하게 된다. \n",
    "    - 준 뉴턴 방식\n",
    "        - 헤시안 계산을 직접 하지 않고 반복적으로 헤시안의 근사행렬을 업데이트하면서 최적화를 수행\n",
    "        - x_(k+1) = x_k - B_k^(-1) * d( f(x_k) ) / dx\n",
    "        - B_k는 헤시안의 근사행렬로, 매 반복마다 이전 기울기 정보들을 이용해 갱신한다.\n",
    "        - DFP, BFGS, L-BFGS등의 방법이 있으며 BFGS가 가장 많이 쓰인다. \n",
    "- Rprop -> 기울기 크기를 무시하고 방향만을 이용하면 단위벡터화시킨걸까?\n",
    "    - 단위벡터가 아닌, 기울기의 부호만 사용하며 크기는 자체적으로 조절시키는 학습모델이다. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
