{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14f808b",
   "metadata": {},
   "source": [
    "# ReLU && Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605d600",
   "metadata": {},
   "source": [
    "## sigmoid의 문제점\n",
    "\n",
    "#### sigmoid를 activation function으로 사용한다면 어떻게 될까?\n",
    "\n",
    "- weight, input를 이용한 sigmoid함수로 output을 구한다. \n",
    "- 실제 정답과의 차이를 이용해 Loss를 구한다. \n",
    "- Loss를 미분하여 gradient를 구한다. \n",
    "- back propagation을 이용해 weight를 업데이트 한다. \n",
    "\n",
    "#### 무엇이 문제일까? \n",
    "\n",
    "- gradient를 구할 때 문제가 된다. \n",
    "- sigmoid함수 그래프를 보면 이해가 쉬워진다. \n",
    "\n",
    "![sigmoid](../statics/image/sigmoid.jpg)\n",
    "\n",
    "- x가 0에 가까울수록 gradient가 확연하게 바뀐다. \n",
    "- 양 끝의 경우, 소수점 뒤의 수만 작게 변하게 된다. \n",
    "- 문제는, back propagation을 이용해 gradient를 전파하고 activation function에서 전파받은 gradient가 무의미해진다(vanishing gradient)\n",
    "- sigmoid함수를 이용하는 레이어가 중첩될수록 더 큰 영향을 끼치게 된다. \n",
    "\n",
    "#### ReLU는 무엇이 다르길래 괜찮은걸까?\n",
    "\n",
    "`f(x) = max(0, x)`\n",
    "\n",
    "- ReLU는 0보다 작으면 무조건 0을, 크면 x를 그대로 반환한다. \n",
    "- x > 0 일 경우, gradient는 항상 1이다. \n",
    "- 음수의 영역에선 gradient가 항상 0이기 떄문에, 음수의 x에 대해선 vanishing gradient가 여전히 발생한다. \n",
    "- pytorch에서는 `torch.nn.relu(x)`를 이용한다. \n",
    "\n",
    "#### 다양한 활성화 함수\n",
    "\n",
    "- sigmoid\n",
    "    - 우리가 아는 그것. 1 / (1 + e^(-x))\n",
    "- tanh\n",
    "    - (e^x - e(-x)) / (e^x + e^(-x))\n",
    "    - sigmoid보다 출력 중심이 0이라 성능을 개선할 수 있다. \n",
    "    - vanishing gradient가 여전히 존재한다. \n",
    "- relu\n",
    "    - 현재 가장 상용화되어있으며, 양수인 x에 항상 사용 가능하다. \n",
    "    - 입력이 0보다 작으면 dead neuron이 발생할 수 있다. \n",
    "- leaky_relu\n",
    "    - f(x) = max(ax, x) a에 따라 음수의 영역에서도 아주 작게나마 gradient를 가진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc17db",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "- 손실함수의 값을 줄이기 위해 가중치를 어떻게 업데이트할까?를 결정하는 역할\n",
    "- torch.optim에 optimizer가 다양하게 존재한다. \n",
    "\n",
    "#### 최적화 알고리즘의 종류\n",
    "- torch.optim.SGD(Stochastic Gradient Descent)\n",
    "    - 한 번에 하나의 샘플 데이터를 이용한다. \n",
    "    - 간단하다\n",
    "    - 학습 튜닝이 필요하며, 수렴까지 오래걸릴 수 있다. \n",
    "- torch.optim.Adagrad\n",
    "    - 과거 기울기를 누적해 learning rate를 자동 조정한다.  \n",
    "    - 학습 데이터가 적을 때 효과적이다. \n",
    "    - lr이 점점 0에 수렴하여 학습을 멈출 수 있다. \n",
    "- torch.optim.Adadelta\n",
    "    - Adadelta를 개선하여, learning rate를 0이 되지 않도록 RMS기반 조정을 수행한다.( Root Mean Square : 기울기 제곱의 평균값의 루트)\n",
    "    - learing rate를 튜닝할 필요가 없다. \n",
    "    - 구조가 복잡하다.\n",
    "- torch.optim.RMSprop\n",
    "    - Adadelta와 유사하다. 최근 기울기에 더 큰 가중치를 둬 학습률을 조정한다. \n",
    "    - RNN에 강하다. (RNN이 뭘까?)\n",
    "    - 모멘텀 조합이 필요할 수 있다. (모멘텀이 뭘까?) \n",
    "- torch.optim.Adam\n",
    "    - 모멘텀 + RMSprop조합으로, 가장 널리 사용된다.\n",
    "    - 수렴이 빠르고 튜닝이 거의 필요 없다. \n",
    "    - 간혹 촤솟값 근처에서 진동형 변동이 일어난다. (와리가리)\n",
    "- torch.optim.Adamax\n",
    "    - Adam 변형으로, 무한노름기반 안정화된 버전이다. (무한노름이 뭘까?)  \n",
    "    - 고차원 공간에 최적화되어있다. \n",
    "    - 일반 Adam보다 사용 빈도가 낮다. \n",
    "- torch.optim.SparseAdam\n",
    "    - sparse tensor용 Adam이다. (sparse tensor가 뭘까?)\n",
    "    - 회소 벡터 최적화에 강하다. \n",
    "    - Dense에는 부적합하다. (Dense가 뭘까?)\n",
    "- torch.optim.ASGD\n",
    "    - 평균 파라미터를 이용한 SGD의 변형버전\n",
    "    - 수렴 안정성이 증가한다. \n",
    "    - 잘 안쓴다. \n",
    "- torch.optim.LBFGS\n",
    "    - 2차 최적화(준 뉴턴 방식), 배치 전체 기반 (준 뉴턴방식이 뭘까?)\n",
    "    - 작은 모델에선 빠른 수렴을 할 수 있다. \n",
    "    - 메모리 부하가 크고, 미니배치에 부적합하다\n",
    "- torch.optim.Rprop\n",
    "    - 기울기 크기 무시하고 방향만을 이용한다(단위 벡터 쓰는건가?)\n",
    "    - 고정된 step size 기반이다. \n",
    "    - SGD보다 느리고, 잘 쓰지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9666d",
   "metadata": {},
   "source": [
    "공부하다보니, 또 다른 모르는 단어들이 나왔다. 한번 찾아봐야겠다. \n",
    "\n",
    "- RNN\n",
    "- 모멘텀\n",
    "- 무한노름\n",
    "- sparse tensor\n",
    "- Dense\n",
    "- 준 뉴턴 방식\n",
    "- Rprop -> 기울기 크기를 무시하고 방향만을 이용하면 단위벡터화시킨걸까?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
